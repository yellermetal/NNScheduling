# -*- coding: utf-8 -*-
"""Schedule.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AruC7DVFbTBcFqaDBq_qMHeii4DLXxIM
"""

import numpy as np

class Job():
  def __init__(self, d = None, r = None):
    def RandDuration(p):
      if np.random.uniform() < p: # Between 1 and 3
        return np.random.randint(1, 4)
      else: # Between 10 and 15
        return np.random.randint(10, 16)
    def RandResource():
      firstRes = np.random.uniform(0.25, 0.5)
      secondRes = np.random.uniform(0.05, 0.1)
      return firstRes
      #if random.random() < 0.5:
      #  return (firstRes, secondRes)
      #else:
      #  return (secondRes, firstRes)
    if d is not None:
      self.duration = d
    else:
      self.duration = RandDuration(0.8)
      
    if r is not None:
      self.resourceReq = r
    else:
      self.resourceReq = RandResource()

  
  def Duration(self):
    return self.duration
  
  def ResourceRequirement(self):
    return self.resourceReq
  

  def __str__(self):
    return "Job Length: " + str(self.duration) + ", ResourceReq: " + str(self.resourceReq)
  
  def __repr__(self):
    return str((self.duration, self.resourceReq))

from collections import deque

class JobSlots():
  def __init__(self, numSlots):
    self.numSlots = numSlots
    self.slots = []
    self.backup = deque([])
    
  def AddJob(self, job):
    if len(self.slots) < self.numSlots:
      self.slots.append(job)
    else:
      self.backup.append(job)
  
  def RemoveJob(self, job):
    if job not in self.slots:
      return False
    self.slots.remove(job)
    if len(self.backup) > 0:
      j = self.backup.popleft()
      self.slots.append(j)
    return True
  
  def IsEmpty(self):
    return (len(self.slots) == 0) and (len(self.backup) == 0)
  
  def Reset(self):
    self.slots = []
    self.backup = deque([])
  
  def Score(self):
    score = 0
    for job in self.slots + list(self.backup):
      score += -1/job.Duration()
    return score
  
  def GetState(self):
    return (self.slots, list(self.backup))

class Cluster():
  def __init__(self, futureSize):
    self.clusterSlots = [0] * futureSize
    self.jobsInSlots = [[]] * futureSize
    self.jobsInCluster = []
    
  def Step(self):
    self.clusterSlots = self.clusterSlots[1:] + [0]
    steppedJobs = self.jobsInSlots[0]
    self.jobsInSlots = self.jobsInSlots[1:] + [[]]
    finishedJobs = []
    for job in steppedJobs:
      if job not in self.jobsInSlots[0]:
        finishedJobs = finishedJobs + [job]
    for job in finishedJobs:
      self.jobsInCluster.remove(job)
    
    
  def Schedule(self, job):
    duration = job.Duration()
    req = job.ResourceRequirement()
    
    for i in range(0, len(self.clusterSlots) - duration + 1):
      flag = True
      for j in range(0, duration):
        if self.clusterSlots[i+j] + req > 1:
          flag = False
      if flag:
        for j in range(0, duration):
          self.clusterSlots[i+j] += req
          self.jobsInSlots[i+j] = self.jobsInSlots[i+j] + [job]
        self.jobsInCluster = self.jobsInCluster + [job]
        return True
    return False
  
  def Score(self):
    score = 0
    for job in self.jobsInCluster:
      score += -1/job.Duration()
    return score
  
  def IsEmpty(self):
    return all(elem == 0 for elem in self.clusterSlots)
  
  def Reset(self):
    for i in range(0, len(self.clusterSlots)):
      self.clusterSlots[i] = 0
  
  def GetState(self):
    return self.clusterSlots

from scipy.stats import bernoulli
import numpy as np

def Sjf(jobsToChoose):
  if len(jobsToChoose) > 0:
      sj = min(jobsToChoose, key = lambda x : x.Duration())
      return sj
  return None

class Manager():
  def __init__(self, futureSize, numSlots, data = None):
    if data is not None:
      self.data = data
    else:
      self.data = bernoulli.rvs(size=10000,p=0.1)
    
    self.cluster = Cluster(futureSize)
    self.jobSlots = JobSlots(numSlots)
    self.jobs_arrived = 0
    self.time = -1
  
  def Schedule(self, job, fromBackup = False):
    (slots, backup) = self.jobSlots.GetState()
    jobsToChoose = slots
    if fromBackup:
       jobsToChoose = slots + backup
    if job not in jobsToChoose:
      return False
    if self.cluster.Schedule(job):
      self.jobSlots.RemoveJob(job)
      return True
    return False
  
  def Step(self):
    self.time += 1
    self.cluster.Step()
    if self.time < len(self.data):
      if self.data[self.time] is not None:
        if self.data[self.time] == 1:
          j = Job()
          self.jobs_arrived += 1
          self.jobSlots.AddJob(j)
        elif self.data[self.time] != 0:
          j = self.data[self.time]
          self.jobs_arrived += 1
          self.jobSlots.AddJob(j)
  
  def Score(self):
    return self.cluster.Score() + self.jobSlots.Score()
  
  def Done(self):
    return self.time >= len(self.data) and self.cluster.IsEmpty() and self.jobSlots.IsEmpty()
  
  def GetState(self):
    return (self.cluster, self.jobSlots)
  
  def NumJobsArrived(self):
    return self.jobs_arrived
  
  def Reset(self):
    self.cluster.Reset()
    self.jobSlots.Reset()
    self.time = 0
    self.jobs_arrived = 0

from scipy.stats import bernoulli
import numpy as np

def CheckSfj(data):
  totalTime = 1000
  m = Manager(20, 10, data)
  time = 0
  averageSlowdown = 0
  while not m.Done() and time < 10 * totalTime:
    m.Step()
    score = m.Score()
    averageSlowdown += -1 * score
    (_, jobSlots) = m.GetState()
    (slots, backup) = jobSlots.GetState()
    job = Sjf(slots + backup)
    if job is not None:
      m.Schedule(job, True)
    time += 1
  
  return averageSlowdown / m.NumJobsArrived()
  
  
  
if __name__ == "__main__":
  totalTime = 1000
  data = bernoulli.rvs(size=totalTime,p=0.3)
  m = Manager(20, 10, data)
  
  time = 0
  averageSlowdown = 0
  while not m.Done():
    m.Step()
    score = m.Score()
    averageSlowdown += -1 * score
    (_, jobSlots) = m.GetState()
    (slots, backup) = jobSlots.GetState()
    job = Sjf(slots + backup)
    if job is not None:
      m.Schedule(job, True)
    time += 1
    if (time % totalTime) == 0:
      (cluster, jobSlots) = m.GetState()
      print("Cluster state: " + str(cluster.GetState()))
      print("Slots state: " + str(jobSlots.GetState()))
  
  print("Average Slowdown: " + str(averageSlowdown / m.NumJobsArrived()))

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

input_size = 41  # TODO change
hidden_size = 64
output_size = 11

USE_GPU = False
dtype = torch.float32

if USE_GPU and torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

from torch.autograd import Variable

def train(model, optimizer, totalTime, val_data, epochs=1):
    """
    Train a model on CIFAR-10 using the PyTorch Module API.
    
    Inputs:
    - model: A PyTorch Module giving the model to train.
    - optimizer: An Optimizer object we will use to train the model
    - epochs: (Optional) A Python integer giving the number of epochs to train for
    
    Returns: Nothing, but prints model accuracies during training.
    """
    for e in range(epochs):
        print("Starting Epoch: " + str(e))
        #for t in range(totalTime):
        t = -1
        data = bernoulli.rvs(size=totalTime,p=0.3)
        m = Manager(20, 10, data)
        for z in range(1000):
          m.Reset()
          model = model.to(device=device)  # move the model parameters to CPU/GPU
          averageSlowdown = 0
          while t < 10 * 1000 and not m.Done():
              t = t+1
              model.train()  # put model to training mode

              (cluster, jobSlots) = m.GetState()
              clusterState = cluster.GetState()
              (slots, backup) = jobSlots.GetState()
              slotsState = []
              for i in range(0,10):
                if i < len(slots):
                  slot = slots[i]
                  slotsState = slotsState + [slot.Duration(), slot.ResourceRequirement()]
                else:
                  slotsState = slotsState + [0, 0]
              backupState = [len(backup)]

              x = torch.Tensor(clusterState + slotsState + backupState)


              #x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU
              #y = y.to(device=device, dtype=torch.long)

              scores = model(x)
              scoresList = scores.tolist()
              #print(scores)
              #print(scores.tolist())
              idx = scoresList.index(max(scoresList))

              calc_loss = False # TODO decide how to know if to calc loss
              reward = 0
              
              if idx < len(slots):
                job = slots[idx]
                if not m.Schedule(job):
                  calc_loss = True
                  m.Step()
                  reward = m.Score()
              else:
                calc_loss = True
                m.Step()
                reward = m.Score()
              
              averageSlowdown += -1 * reward
              # Zero out all of the gradients for the variables which the optimizer
              # will update.
              optimizer.zero_grad()

              if calc_loss and reward != 0:
                  y = scores.clone().detach()
                  #print(y)
                  y[0] = y[0] + (1/(-1 * reward))
                  #print(y)
                  
                  #print("Score")
                  #print(reward)
                  
                  #print(y)
                  # TODO Alter loss
                  loss = F.mse_loss(scores, y)  # l2 loss
                  
                  #print("Loss")
                  #print(loss)
                  #loss = Variable(torch.FloatTensor([loss]), requires_grad=True)
                  #print(loss)
                  # This is the backwards pass: compute the gradient of the loss with
                  # respect to each  parameter of the model.
                  loss.backward()

                  # Actually update the parameters of the model using the gradients
                  # computed by the backwards pass.
                  optimizer.step()

              #if t % 1000 == 0:
                  #print('Iteration %d, loss = %.4f' % (t, loss.item()))
                  #check_accuracy(val_data, model)
                  #print("Current time: " + str(t))
        
        print("Finished epoch: " + str(e))
        check_accuracy(model)
        sjfSlowdown = CheckSfj(data)
        print("SJF Average slowdown : " + str(sjfSlowdown))

def check_accuracy(model):
    data = bernoulli.rvs(size=totalTime,p=0.3)
    m = Manager(20, 10, data)
    t = -1
    averageSlowdown = 0
    while t < 10000 and not m.Done():
      model.eval()  # put model to training mode

      (cluster, jobSlots) = m.GetState()
      clusterState = cluster.GetState()
      (slots, backup) = jobSlots.GetState()
      slotsState = []
      for i in range(0,10):
        if i < len(slots):
          slot = slots[i]
          slotsState = slotsState + [slot.Duration(), slot.ResourceRequirement()]
        else:
          slotsState = slotsState + [0, 0]
      backupState = [len(backup)]

      x = torch.Tensor(clusterState + slotsState + backupState)



      scores = model(x)
      scoresList = scores.tolist()

      idx = scoresList.index(max(scoresList))

      loss = 0
      if idx < len(slots):
        job = slots[idx]
        if not m.Schedule(job):
          m.Step()
          loss = m.Score()
      else:
        m.Step()
        loss = m.Score()
            
      t += 1
      averageSlowdown += -1 * loss
    print("Average slowdown : " + str(averageSlowdown / m.NumJobsArrived()))

class FourLayerFC(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        # assign layer objects to class attributes
        self.fc1 = nn.Linear(input_size, hidden_size)
        # nn.init package contains convenient initialization methods
        # http://pytorch.org/docs/master/nn.html#torch-nn-init 
        nn.init.kaiming_normal_(self.fc1.weight)
        
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        nn.init.kaiming_normal_(self.fc2.weight)
        
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        nn.init.kaiming_normal_(self.fc3.weight)
        
        self.fc4 = nn.Linear(hidden_size, output_size)
        nn.init.kaiming_normal_(self.fc4.weight)
    
    def forward(self, x):
        scores = self.fc4(F.relu(self.fc3(F.relu(self.fc2(F.relu(self.fc1(x)))))))
        return scores

train_data = [] # TODO fill, list of tuples of x,y
val_data = [] # TODO fill
model = FourLayerFC(input_size, hidden_size, output_size)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
#for param in model.parameters():
#  param.requires_grad = False
#  param.allow_unreachable = True
train(model, optimizer, 1000, val_data, epochs=100)